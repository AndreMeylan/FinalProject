{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "507a4fa1",
   "metadata": {},
   "source": [
    "# Diabetes Prediction: MIBA Sec B Team 6\n",
    "\n",
    "**Goal:** Based on indicators related to a person's physical health, we would like to predict if a person is at risk of developing diabetes. \n",
    "\n",
    "The business case and reasoning behind this is explained in our presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7526f8ea",
   "metadata": {},
   "source": [
    "## Initital preparation of data and environment\n",
    "\n",
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "855cc467-1e72-42fa-9488-928995725b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3fs is already installed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import s3fs\n",
    "    print(\"s3fs is already installed.\")\n",
    "except ImportError:\n",
    "    print(\"s3fs not found, installing...\")\n",
    "    %pip install s3fs --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "27a96d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import tarfile\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46cd2de-4602-4e91-a485-c0b827db3072",
   "metadata": {},
   "source": [
    "### Check for new dataset upload in S3 bucket\n",
    "\n",
    "In the following, we **define a loop in order to check if a new dataset has been uploaded to the S3 bucket** within the last 24 hours. \n",
    "\n",
    "- We establish a connection to AWS S3 using boto3, specifying a region and a bucket name, to interact with stored files.\n",
    "- We define the funciton get_latest_file to paginate through the objects in the specified S3 bucket, identify, and return metadata of the latest modified file using a paginator.\n",
    "- We then implement the function process_new_file to download the latest file identified by get_latest_file from S3 to local storage, then read it into a pandas DataFrame, and return this DataFrame.\n",
    "- We set up a loop to continuously check for the latest file in the S3 bucket that has been modified within the last 24 hours. This loop downloads the new file using process_new_file, reads it into a DataFrame, and breaks if a new file is successfully processed or no new files are found.\n",
    "- We also include error handling to manage exceptions during the file retrieval and processing phases, ensuring robustness in operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bbe4ebea-f045-4134-8b11-5a5c9deea5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File diabetes_prediction_dataset.csv downloaded successfully.\n",
      "Latest CSV file read into DataFrame 'df_diabetes'.\n",
      "   gender   age  hypertension  heart_disease smoking_history    bmi  \\\n",
      "0  Female  80.0             0              1           never  25.19   \n",
      "1  Female  54.0             0              0         No Info  27.32   \n",
      "2    Male  28.0             0              0           never  27.32   \n",
      "3  Female  36.0             0              0         current  23.45   \n",
      "4    Male  76.0             1              1         current  20.14   \n",
      "\n",
      "   HbA1c_level  blood_glucose_level  diabetes  \n",
      "0          6.6                  140         0  \n",
      "1          6.6                   80         0  \n",
      "2          5.7                  158         0  \n",
      "3          5.0                  155         0  \n",
      "4          4.8                  155         0  \n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "import os  # For file handling\n",
    "\n",
    "# Configure AWS region and bucket name\n",
    "s3_client = boto3.client('s3', region_name='eu-north-1')\n",
    "bucket_name = 'prepreocesseddiabetescpfp'\n",
    "\n",
    "def get_latest_file():\n",
    "    try:\n",
    "        paginator = s3_client.get_paginator('list_objects_v2')\n",
    "        for page in paginator.paginate(Bucket=bucket_name):\n",
    "            if \"Contents\" in page:\n",
    "                latest_file = max(page['Contents'], key=lambda x: x['LastModified'])\n",
    "                return latest_file\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting latest file: {e}\")\n",
    "    return None\n",
    "\n",
    "def process_new_file(file_info):\n",
    "    local_file_name = file_info['Key']\n",
    "    try:\n",
    "        # Download the file from S3 to local storage\n",
    "        s3_client.download_file(bucket_name, file_info['Key'], local_file_name)\n",
    "        print(f\"File {local_file_name} downloaded successfully.\")\n",
    "        \n",
    "        # Read the downloaded file into a DataFrame and return it\n",
    "        df_diabetes = pd.read_csv(local_file_name)\n",
    "        print(\"Latest CSV file read into DataFrame 'df_diabetes'.\")\n",
    "        print(df_diabetes.head())\n",
    "        \n",
    "        return df_diabetes\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Existing loop code\n",
    "df_diabetes = None  # Initialize df_diabetes before the loop\n",
    "while True:\n",
    "    latest_file = get_latest_file()\n",
    "    if latest_file:\n",
    "        # Check if the latest file was modified within the last 24 hours\n",
    "        # Your existing condition here...\n",
    "        df_diabetes = process_new_file(latest_file)  # Capture the returned DataFrame\n",
    "        if df_diabetes is not None:\n",
    "            break\n",
    "    else:\n",
    "        print(\"No new files found in the last 24 hours.\")\n",
    "        break  # Break the loop if no files are found\n",
    "    time.sleep(60)  # Adjust the sleep time as needed\n",
    "\n",
    "# After the loop, check if df_diabetes is not None before proceeding\n",
    "if df_diabetes is not None:\n",
    "    # Proceed with your data processing using df_diabetes\n",
    "    data = df_diabetes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "157ee3fe",
   "metadata": {},
   "source": [
    "### Data Loading & preprocessing pipeline definition\n",
    "\n",
    "Based on the check before for the upload of a **new dataset into the S3 bucket**, we import this dataset into our notebook. \n",
    "The dataset was originally gathered from Kaggle, specifically from here: https://www.kaggle.com/datasets/iammustafatz/diabetes-prediction-dataset/data.\n",
    "But this type of data could potentially also be available to an a healthcare system like the Spanish one, on which our business case is based.\n",
    "\n",
    "We then conduct the following **preprocessing steps** according to what we have discovered to be necessary when inspecting the dataset:\n",
    "- We separate the dataset into features (X) and the target variable (y), where 'diabetes' is the target variable we aim to predict.\n",
    "- We define a preprocessing pipeline for numerical features, which includes filling missing values with the mean of the column and then scaling the data to have a mean of 0 and a standard deviation of 1.\n",
    "- We also define a preprocessing pipeline for categorical features, where missing values are filled with the most frequent value in the column, and then we apply one-hot encoding to transform categorical variables into a form that could be provided to ML algorithms.\n",
    "We identify the categorical and numerical columns in our dataset, specifically naming 'gender' and 'smoking_history' as categorical, and 'age', 'hypertension', 'heart_disease', 'bmi', 'HbA1c_level', and 'blood_glucose_level' as numerical.\n",
    "Finally, we combine these preprocessing steps into a single ColumnTransformer. This allows us to apply the specific transformations to the numerical and categorical columns, respectively, while passing through any other columns unmodified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "12e0ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_diabetes\n",
    "\n",
    "# Store prediction target in separate variable\n",
    "X = data.drop('diabetes', axis=1)\n",
    "y = data['diabetes']\n",
    "\n",
    "# Define the preprocessing for numerical columns:\n",
    "# SimpleImputer to fill missing values with the mean, then scaling the data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define the preprocessing for categorical columns:\n",
    "# Filling missing values with the most frequent value then applying one-hot encoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Define categorical and numerical columns\n",
    "categorical_cols = [\n",
    "    'gender', 'smoking_history'\n",
    "]\n",
    "numerical_cols = ['age', 'hypertension', 'heart_disease', 'bmi', 'HbA1c_level', 'blood_glucose_level']\n",
    "\n",
    "# Create the ColumnTransformer with both transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee96395-c41b-4efd-85a8-5031f3fb85b2",
   "metadata": {},
   "source": [
    "We quickly **inspect shape and overall structure of the data** to ensure everything looks as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b2b68923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 9)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f30ae622-fd9e-443e-abb6-bd508581db96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>smoking_history</th>\n",
       "      <th>bmi</th>\n",
       "      <th>HbA1c_level</th>\n",
       "      <th>blood_glucose_level</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>never</td>\n",
       "      <td>25.19</td>\n",
       "      <td>6.6</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Female</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No Info</td>\n",
       "      <td>27.32</td>\n",
       "      <td>6.6</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>never</td>\n",
       "      <td>27.32</td>\n",
       "      <td>5.7</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>current</td>\n",
       "      <td>23.45</td>\n",
       "      <td>5.0</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>current</td>\n",
       "      <td>20.14</td>\n",
       "      <td>4.8</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender   age  hypertension  heart_disease smoking_history    bmi  \\\n",
       "0  Female  80.0             0              1           never  25.19   \n",
       "1  Female  54.0             0              0         No Info  27.32   \n",
       "2    Male  28.0             0              0           never  27.32   \n",
       "3  Female  36.0             0              0         current  23.45   \n",
       "4    Male  76.0             1              1         current  20.14   \n",
       "\n",
       "   HbA1c_level  blood_glucose_level  diabetes  \n",
       "0          6.6                  140         0  \n",
       "1          6.6                   80         0  \n",
       "2          5.7                  158         0  \n",
       "3          5.0                  155         0  \n",
       "4          4.8                  155         0  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd32f2",
   "metadata": {},
   "source": [
    "### Train / val / test split & transformation\n",
    "\n",
    "- We **split the dataset into training, validation, and test sets** using an 80/20 split for the initial separation of training and testing/validation data, and then a 50/50 split to further divide the test/validation data into validation and test sets. The random_state parameter ensures that our splits are reproducible.\n",
    "- We apply the previously defined preprocessing steps to the training, validation, and test sets. This involves fitting the preprocessing pipeline to the training data to learn the necessary transformations and then transforming the training data based on this fit.\n",
    "- The validation and test sets are transformed using the same preprocessing steps as the training set, but without fitting the preprocessing pipeline again. This ensures that all sets are transformed consistently and prevents data leakage from the validation and test sets into the model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d5bfc6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and test sets\n",
    "X_train, X_testval, y_train, y_testval = train_test_split(X, y, train_size=0.8, random_state=1200)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_testval, y_testval, train_size=0.5, random_state=1200)\n",
    "\n",
    "# Apply preprocessing to the training, validation, and test sets:\n",
    "\n",
    "# Fit and transform the training set\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation and test set\n",
    "X_val_preprocessed = preprocessor.transform(X_val)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7886ddc7-86e3-4287-b06e-8f7872a34b34",
   "metadata": {},
   "source": [
    "After the split, we check for the shape to ensure the operation and transformation worked correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b4f5b29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80000, 15), (10000, 15), (10000, 15))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_preprocessed.shape, X_val_preprocessed.shape, X_test_preprocessed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9156cf6-b0e3-4604-bbf8-a6f76de00c1c",
   "metadata": {},
   "source": [
    "### Transform preprocessed data\n",
    "\n",
    "Since the feature names were lost during the preprocessing, we are now adding them back to the dataset through our preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2b427ce5-f705-47aa-8137-a6e97af44ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diabetes</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>bmi</th>\n",
       "      <th>HbA1c_level</th>\n",
       "      <th>blood_glucose_level</th>\n",
       "      <th>gender_Female</th>\n",
       "      <th>gender_Male</th>\n",
       "      <th>gender_Other</th>\n",
       "      <th>smoking_history_No Info</th>\n",
       "      <th>smoking_history_current</th>\n",
       "      <th>smoking_history_ever</th>\n",
       "      <th>smoking_history_former</th>\n",
       "      <th>smoking_history_never</th>\n",
       "      <th>smoking_history_not current</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.081830</td>\n",
       "      <td>-0.284208</td>\n",
       "      <td>-0.202524</td>\n",
       "      <td>-0.001838</td>\n",
       "      <td>-0.492907</td>\n",
       "      <td>-0.295076</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.695130</td>\n",
       "      <td>-0.284208</td>\n",
       "      <td>4.937683</td>\n",
       "      <td>-0.001838</td>\n",
       "      <td>0.441547</td>\n",
       "      <td>-0.295076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.362410</td>\n",
       "      <td>-0.284208</td>\n",
       "      <td>-0.202524</td>\n",
       "      <td>-0.001838</td>\n",
       "      <td>0.161211</td>\n",
       "      <td>0.540355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.406834</td>\n",
       "      <td>3.518546</td>\n",
       "      <td>-0.202524</td>\n",
       "      <td>-0.458663</td>\n",
       "      <td>1.002220</td>\n",
       "      <td>-0.196790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.695130</td>\n",
       "      <td>-0.284208</td>\n",
       "      <td>-0.202524</td>\n",
       "      <td>-0.001838</td>\n",
       "      <td>0.628438</td>\n",
       "      <td>0.515784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79995</th>\n",
       "      <td>0</td>\n",
       "      <td>0.895498</td>\n",
       "      <td>-0.284208</td>\n",
       "      <td>-0.202524</td>\n",
       "      <td>1.137219</td>\n",
       "      <td>1.002220</td>\n",
       "      <td>-1.302507</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79996</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.481645</td>\n",
       "      <td>-0.284208</td>\n",
       "      <td>-0.202524</td>\n",
       "      <td>1.858522</td>\n",
       "      <td>-1.894589</td>\n",
       "      <td>-0.295076</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79997</th>\n",
       "      <td>0</td>\n",
       "      <td>1.295314</td>\n",
       "      <td>-0.284208</td>\n",
       "      <td>-0.202524</td>\n",
       "      <td>-0.001838</td>\n",
       "      <td>0.441547</td>\n",
       "      <td>0.491212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79998</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.785933</td>\n",
       "      <td>-0.284208</td>\n",
       "      <td>-0.202524</td>\n",
       "      <td>-1.083792</td>\n",
       "      <td>-1.894589</td>\n",
       "      <td>-1.425364</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79999</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.236853</td>\n",
       "      <td>-0.284208</td>\n",
       "      <td>-0.202524</td>\n",
       "      <td>1.648142</td>\n",
       "      <td>1.002220</td>\n",
       "      <td>0.171783</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       diabetes       age  hypertension  heart_disease       bmi  HbA1c_level  \\\n",
       "0             0 -0.081830     -0.284208      -0.202524 -0.001838    -0.492907   \n",
       "1             0  1.695130     -0.284208       4.937683 -0.001838     0.441547   \n",
       "2             0  0.362410     -0.284208      -0.202524 -0.001838     0.161211   \n",
       "3             0  0.406834      3.518546      -0.202524 -0.458663     1.002220   \n",
       "4             0  1.695130     -0.284208      -0.202524 -0.001838     0.628438   \n",
       "...         ...       ...           ...            ...       ...          ...   \n",
       "79995         0  0.895498     -0.284208      -0.202524  1.137219     1.002220   \n",
       "79996         0 -0.481645     -0.284208      -0.202524  1.858522    -1.894589   \n",
       "79997         0  1.295314     -0.284208      -0.202524 -0.001838     0.441547   \n",
       "79998         0 -1.785933     -0.284208      -0.202524 -1.083792    -1.894589   \n",
       "79999         0 -1.236853     -0.284208      -0.202524  1.648142     1.002220   \n",
       "\n",
       "       blood_glucose_level  gender_Female  gender_Male  gender_Other  \\\n",
       "0                -0.295076            1.0          0.0           0.0   \n",
       "1                -0.295076            0.0          1.0           0.0   \n",
       "2                 0.540355            0.0          1.0           0.0   \n",
       "3                -0.196790            0.0          1.0           0.0   \n",
       "4                 0.515784            1.0          0.0           0.0   \n",
       "...                    ...            ...          ...           ...   \n",
       "79995            -1.302507            0.0          1.0           0.0   \n",
       "79996            -0.295076            1.0          0.0           0.0   \n",
       "79997             0.491212            0.0          1.0           0.0   \n",
       "79998            -1.425364            1.0          0.0           0.0   \n",
       "79999             0.171783            1.0          0.0           0.0   \n",
       "\n",
       "       smoking_history_No Info  smoking_history_current  smoking_history_ever  \\\n",
       "0                          1.0                      0.0                   0.0   \n",
       "1                          0.0                      0.0                   0.0   \n",
       "2                          0.0                      0.0                   0.0   \n",
       "3                          0.0                      0.0                   1.0   \n",
       "4                          0.0                      0.0                   1.0   \n",
       "...                        ...                      ...                   ...   \n",
       "79995                      0.0                      0.0                   0.0   \n",
       "79996                      0.0                      0.0                   1.0   \n",
       "79997                      0.0                      0.0                   0.0   \n",
       "79998                      1.0                      0.0                   0.0   \n",
       "79999                      0.0                      0.0                   0.0   \n",
       "\n",
       "       smoking_history_former  smoking_history_never  \\\n",
       "0                         0.0                    0.0   \n",
       "1                         1.0                    0.0   \n",
       "2                         0.0                    1.0   \n",
       "3                         0.0                    0.0   \n",
       "4                         0.0                    0.0   \n",
       "...                       ...                    ...   \n",
       "79995                     0.0                    1.0   \n",
       "79996                     0.0                    0.0   \n",
       "79997                     0.0                    0.0   \n",
       "79998                     0.0                    0.0   \n",
       "79999                     0.0                    1.0   \n",
       "\n",
       "       smoking_history_not current  \n",
       "0                              0.0  \n",
       "1                              0.0  \n",
       "2                              0.0  \n",
       "3                              0.0  \n",
       "4                              0.0  \n",
       "...                            ...  \n",
       "79995                          0.0  \n",
       "79996                          0.0  \n",
       "79997                          1.0  \n",
       "79998                          0.0  \n",
       "79999                          0.0  \n",
       "\n",
       "[80000 rows x 16 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the numerical features, there is no change necessary in the naming of the columns\n",
    "numerical_features = numerical_cols\n",
    "\n",
    "# For the categorical features, we have to get the new column names from the one-hot encoder\n",
    "categorical_features = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_cols)\n",
    "\n",
    "# We then need to combine the feature names from both transformations\n",
    "all_features = list(numerical_features) + list(categorical_features)\n",
    "\n",
    "# Create DataFrames with the proper column names\n",
    "X_train_preprocessed = pd.DataFrame(X_train_preprocessed, columns=all_features)\n",
    "X_val_preprocessed = pd.DataFrame(X_val_preprocessed, columns=all_features)\n",
    "X_test_preprocessed = pd.DataFrame(X_test_preprocessed, columns=all_features)\n",
    "\n",
    "# Recombine X and y for each set before uploading to S3, because SageMaker expects the prediction target to be the first column\n",
    "train_preprocessed = pd.DataFrame(X_train_preprocessed)\n",
    "train_preprocessed.insert(0, 'diabetes', y_train.values)\n",
    "\n",
    "val_preprocessed = pd.DataFrame(X_val_preprocessed)\n",
    "val_preprocessed.insert(0, 'diabetes', y_val.values)\n",
    "\n",
    "test_preprocessed = pd.DataFrame(X_test_preprocessed)\n",
    "test_preprocessed.insert(0, 'diabetes', y_test.values)\n",
    "\n",
    "train_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba1f7b3",
   "metadata": {},
   "source": [
    "### Define S3 upload function\n",
    "\n",
    "To work with Sagemaker, we are now uploading our data to S3.\n",
    "\n",
    "- We initialize a resource instance for Amazon S3 using the boto3 library. This instance will allow us to interact with our S3.\n",
    "- We define a function upload_to_s3 that takes a pandas DataFrame, a bucket name, and a filename as its arguments. This function is designed to upload the DataFrame to our S3 bucket.\n",
    "Inside the function, we first convert the DataFrame to a CSV format string without headers and index columns. This conversion is done in memory using io.StringIO, which acts as a temporary placeholder for the CSV string.\n",
    "- We then create an object in the specified S3 bucket with the given filename and upload the CSV string to S3 by calling the put method on the S3 object. This operation saves the DataFrame as a CSV file in the specified S3 bucket under the specified filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1af200a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "\n",
    "def upload_to_s3(df, bucket, filename):\n",
    "    placeholder = io.StringIO()\n",
    "    df.to_csv(placeholder, header=False, index=False)\n",
    "    # Upload csv string to S3\n",
    "    object = s3.Object(bucket, filename)\n",
    "    object.put(Body=placeholder.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d64d4fc",
   "metadata": {},
   "source": [
    "After defining this, we upload the train and validation split to another S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "30701f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_to_s3(train_preprocessed, 'diabeterstraindata', 'sagemaker-data/kc/train.csv')\n",
    "upload_to_s3(val_preprocessed, 'diabeterstraindata', 'sagemaker-data/kc/val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a62b57d-1422-429a-bd27-d4321a08ac05",
   "metadata": {},
   "source": [
    "We now verify these files have been uploaded to S3 by checking our S3 bucket using the AWS console."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66910419",
   "metadata": {},
   "source": [
    "## Setting up the model (XGBoost)\n",
    "\n",
    "Before we dive into the steps we have taken to set up the model, we quickly want to explain why we decided to choose **XGBoost** (eXtreme Gradient Boosting) as a predictive model:\n",
    "\n",
    "#### 1. High Predictive Accuracy\n",
    "\n",
    "- XGBoost is renowned for its ability to deliver highly accurate predictions.\n",
    "- This accuracy is critical in correctly identifying individuals at risk of developing diabetes, ensuring that interventions are appropriately targeted.\n",
    "\n",
    "#### 2. Complex Relationship Modeling\n",
    "\n",
    "- The risk factors for diabetes are numerous and complex, including lifestyle, genetic factors, and other health conditions.\n",
    "- XGBoost excels at uncovering nonlinear relationships and interactions between these factors, making it particularly suited for this application.\n",
    "\n",
    "#### 3. Efficiency with Large Datasets\n",
    "\n",
    "- Healthcare data often encompasses extensive records with mulitple features.\n",
    "- XGBoost is designed to be efficient and scalable, making it capable of handling the volume and complexity of healthcare datasets without compromising on speed or performance - this would also make it possible in the future to include more features should more types of data be collected.\n",
    "\n",
    "#### 4. Feature Importance Insights\n",
    "\n",
    "- Understanding which factors contribute most significantly to diabetes risk is crucial for prevention efforts.\n",
    "- XGBoost provides feature importance scores, offering valuable insights into the most impactful risk factors.\n",
    "- This information can guide healthcare providers in designing more effective diabetes prevention programs.\n",
    "\n",
    "#### 5. Flexibility and Customization\n",
    "\n",
    "- XGBoost offers a wide range of hyperparameters that can be tuned to optimize performance for specific datasets.\n",
    "- This flexibility allows us to tailor the model to best fit the unique characteristics of the data related to diabetes risk.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "Given its predictive power, efficiency, and the actionable insights it can provide, XGBoost stands out as the optimal choice for developing a model to predict diabetes risk. By implementing this model, we aim to significantly reduce the incidence of diabetes and alleviate its financial burden on the Spanish National Health System, ultimately contributing to healthier communities and more sustainable healthcare spending.\n",
    "We use the class `Estimator` from the `sagemaker.estimator` module. That will create the **environment** to run  training jobs for a model.\n",
    "\n",
    "We specify: \n",
    "\n",
    "- A container name (Sagemaker works with containers. This code is pointing to a pre-existing container that holds everything that is needed to run xgboost. \n",
    "- A role name (the training job needs a role to have sufficient permissions, similarly to what we saw in Lambda functions). Remember that we created this role when starting the notebook server. \n",
    "- The number of instances for training (we use 1 but could use more in large jobs, to scale). \n",
    "- The type of instance (we select one that's included in the Sagemaker Free Tier). \n",
    "- The output path, where the model and other info will be written\n",
    "- The hyperparameters of the algorithm (number of training rounds and loss function)\n",
    "- The current session (it needs that for internal purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "158fbed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: cpu.\n"
     ]
    }
   ],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "region_name = boto3.Session().region_name\n",
    "container = sagemaker.image_uris.retrieve('xgboost', region_name, version='0.90-1')\n",
    "output_location = 's3://diabetesstorage/sagemaker-output/'\n",
    "\n",
    "hyperparams = {\n",
    "    'eval_metric': 'auc,logloss,error',\n",
    "    'num_round': '20',\n",
    "    'objective': 'binary:logistic'\n",
    "}\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',  # Adjusted to a supported instance type\n",
    "    output_path=output_location,\n",
    "    hyperparameters=hyperparams,\n",
    "    sagemaker_session=sagemaker.Session()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8722b5c6",
   "metadata": {},
   "source": [
    "Now we create **\"channels\"**. We need to specify where the data is located and in which format in a specific dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2e5c0f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = TrainingInput(\n",
    "    s3_data='s3://diabeterstraindata/sagemaker-data/kc/train.csv',\n",
    "    content_type='text/csv'\n",
    ")\n",
    "val_channel = TrainingInput(\n",
    "    s3_data='s3://diabeterstraindata/sagemaker-data/kc/val.csv',\n",
    "    content_type='text/csv'\n",
    ")\n",
    "\n",
    "channels_for_training = {\n",
    "    'train': train_channel,\n",
    "    'validation': val_channel\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937e5471",
   "metadata": {},
   "source": [
    "## Train model (XGBoost)\n",
    "\n",
    "Now, a training job will be launched.\n",
    "\n",
    "- We call the fit method on an estimator object, passing it channels_for_training as the input. This input  specifies the data sources where the training data is located. By setting logs=False, we opt not to show the logs generated during the training process. This step initiates the training of the model using the provided data channels.\n",
    "- We access the _current_job_name attribute of the estimator object. This attribute  holds the name or identifier of the current training job. Accessing this attribute after initiating a training job allows us to retrieve the unique name or identifier associated with that specific training session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9d0087c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-04-02-20-09-54-006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2024-04-02 20:09:54 Starting - Starting the training job..\n",
      "2024-04-02 20:10:09 Starting - Preparing the instances for training...\n",
      "2024-04-02 20:10:33 Downloading - Downloading input data...\n",
      "2024-04-02 20:10:53 Downloading - Downloading the training image....\n",
      "2024-04-02 20:11:18 Training - Training image download completed. Training in progress....\n",
      "2024-04-02 20:11:39 Uploading - Uploading generated training model.\n",
      "2024-04-02 20:11:50 Completed - Training job completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sagemaker-xgboost-2024-04-02-20-09-54-006'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.fit(inputs=channels_for_training, logs=False)\n",
    "\n",
    "estimator._current_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f92cbd-908e-4482-8b87-33da5f1c6d35",
   "metadata": {},
   "source": [
    "### Retrieve metrics of the model (XGBoost)\n",
    "\n",
    "We considered the following parameters for evaluating the model: \n",
    "- **AUC:** We use this metric to measure the ability of a classifier to distinguish between classes. An AUC of 1 represents a perfect model, while an AUC of 0.5 suggests no discriminative power, equivalent to random guessing.\n",
    "  - The train:auc indicates the AUC on the training dataset, and validation:auc shows the AUC on a validation dataset. High values on both metrics indicate that the model has a good predictive performance and can distinguish between the positive and negative classes well.\n",
    "- **Logloss:** Logarithmic loss (log loss) measures the performance of a classification model where the prediction is a probability between 0 and 1. The goal is to minimize this value. Lower log loss values are better, with 0 representing a perfect log loss.\n",
    "    - The train:logloss is the log loss on the training dataset, while the validation:logloss is on the validation dataset. Consistently low log loss values on both training and validation suggest that the model is performing well and is not overfitting.\n",
    "- **Error:** The classification error metrics represent the fraction of predictions that were incorrect. The goal is to minimize this metric. A low training error is expected, but what's more important is the validation error, as it indicates how well the model is expected to perform on unseen data.\n",
    "    - If the validation error is significantly higher than the training error, it could indicate that the model may not generalize well.\n",
    "\n",
    "Evaluating these metrics together gives us a more comprehensive understanding of the model's performance. We want to see high AUC values along with low log loss and error values on both training and validation datasets. It's also important that the training and validation metrics are close to each other, which suggests that the model generalizes well and is not overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "097aea4c-75c8-4be6-936c-aecf6d593830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>train:auc</td>\n",
       "      <td>0.978681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>validation:auc</td>\n",
       "      <td>0.976233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>train:logloss</td>\n",
       "      <td>0.086373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>validation:logloss</td>\n",
       "      <td>0.091624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>train:error</td>\n",
       "      <td>0.027681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>validation:error</td>\n",
       "      <td>0.029700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp         metric_name     value\n",
       "0        0.0           train:auc  0.978681\n",
       "1        0.0      validation:auc  0.976233\n",
       "2        0.0       train:logloss  0.086373\n",
       "3        0.0  validation:logloss  0.091624\n",
       "4        0.0         train:error  0.027681\n",
       "5        0.0    validation:error  0.029700"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get metrics of model\n",
    "metrics = sagemaker.analytics.TrainingJobAnalytics(\n",
    "    estimator._current_job_name,\n",
    "    metric_names=['train:auc', 'validation:auc', 'train:logloss', 'validation:logloss', 'train:error', 'validation:error']\n",
    ")\n",
    "metrics.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bde1d7-fbbd-4cfd-98e7-a9d4cd9cba18",
   "metadata": {},
   "source": [
    "One of the cool things XGBoost normally enables is determining the **feature importance** which can help us determine which features are actually relevant and important to our prediction model. After multiple tries, we have unfortunately not been able to make this work, as it exceeds our knowwledge from the course. Yet, we wanted to document in the following our effort to trying this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e68aa85b-ad56-41ad-836b-5ac2c3038c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature importance\n",
    "#### Get feature importance form the model\n",
    "# model_artifact_s3_uri = f'{output_location}/{estimator.latest_training_job.name}/output/model.tar.gz'\n",
    "#### Define a local path to download the model artifact\n",
    "# local_path = 'downloaded_model.tar.gz'\n",
    "#### Download the model artifact from S3\n",
    "# S3Downloader.download(s3_uri=model_artifact_s3_uri, local_path=local_path)\n",
    "#### Extract the model artifact\n",
    "# with tarfile.open(local_path) as tar:\n",
    "#    tar.extractall()\n",
    "# model_bin_file = [file for file in os.listdir('.') if file.endswith('.model') or file.endswith('.bin')][0]\n",
    "#### Load the model\n",
    "# xgboost_model = xgb.Booster()\n",
    "# xgboost_model.load_model(model_bin_file)\n",
    "#### Plot the feature importance\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plot_importance(xgboost_model)\n",
    "# plt.title(\"Feature Importance\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db45b229-36e6-4258-a143-e3bffbbca7af",
   "metadata": {},
   "source": [
    "### Interpretation of metrics\n",
    "\n",
    "- **AUC (Area Under the Curve):**\n",
    "    - train:auc: 0.978681 - This is a high AUC value for the training set, which suggests that our model has a strong discriminative power between the positive and negative cases in the training data.\n",
    "    - validation:auc: 0.976233 - Our validation AUC is also high and quite close to the training AUC, indicating that our model's ability to differentiate between classes generalizes well to the validation set.\n",
    "- **Log Loss:**\n",
    "    - train:logloss: 0.086373 - The training log loss is low, indicating a good fit of our model on the training data. Our model gives well-calibrated probabilities for the training set.\n",
    "    - validation:logloss: 0.091624 - Our validation log loss is only slightly higher than the training log loss. This suggests that our model's predictions on the validation set are also well-calibrated and that the model is likely not overfitting.\n",
    "- **Error:**\n",
    "    - train:error: 0.027681 - The low training error rate indicates that our model is accurate in its predictions on the training set.\n",
    "    validation:error: 0.029700 - The validation error is very close to the training error, which is a positive sign. It means our model is achieving similar accuracy on the validation set, which is not seen during training.\n",
    "\n",
    "Overall, these metrics suggest that our XGBoost model is performing well. The similar values for training and validation metrics indicate that the model is generalizing well to unseen data, without signs of overfitting. The high AUC values suggest strong classification capability, the low log loss indicates confidence in predictions, and the low error rates reflect high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d6f8e3-03b6-42bd-94ce-f2c1913cd9ad",
   "metadata": {},
   "source": [
    "## Model tuning\n",
    "\n",
    "- We import necessary classes from the SageMaker Python SDK, including various parameter types for hyperparameter tuning and the TrainingInput class for specifying training data inputs.\n",
    "- We define a range of values for various hyperparameters that will be explored during the hyperparameter tuning process. These parameters include both continuous values (like 'eta', 'min_child_weight', 'alpha', etc.) and integer values ('max_depth'). The aim is to find the best combination of these hyperparameters that optimizes the model's performance.\n",
    "- We set specific hyperparameters on the estimator object, such as eval_metric, num_round, objective, and early_stopping_rounds. These settings include the evaluation metric to be optimized, the number of training rounds, the objective function of the model (binary logistic regression in this case), and the number of rounds with no improvement to wait before stopping training early.\n",
    "- We specify the objective metric name ('validation:auc') that the hyperparameter tuning job should optimize for. This is the metric that SageMaker's hyperparameter tuner will attempt to maximize or minimize.\n",
    "- We create a HyperparameterTuner object by providing the estimator, the objective metric name, the hyperparameter ranges, and limits on the number of jobs and parallel jobs. This setup configures how the hyperparameter tuning process will be executed.\n",
    "- We prepare the training and validation datasets by specifying their locations in S3 and their content type (CSV in this case), using the TrainingInput class.\n",
    "- Finally, we launch the hyperparameter tuning job by calling the fit method on the tuner object and passing it the training and validation inputs. This step initiates the search for the best hyperparameter values within the specified ranges by training multiple models in parallel, as configured, and evaluating their performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ed60d885-b560-4b5a-8a5c-240b0b0d2b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating hyperparameter tuning job with name: sagemaker-xgboost-240402-2011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................!\n"
     ]
    }
   ],
   "source": [
    "# Specify the hyperparameter ranges\n",
    "hyperparameter_ranges = {\n",
    "    'eta': ContinuousParameter(0.01, 0.2),\n",
    "    'min_child_weight': ContinuousParameter(1, 10),\n",
    "    'alpha': ContinuousParameter(0, 2),\n",
    "    'max_depth': IntegerParameter(3, 10),\n",
    "    'subsample': ContinuousParameter(0.5, 1),\n",
    "    'colsample_bytree': ContinuousParameter(0.5, 1),\n",
    "    'gamma': ContinuousParameter(0, 5),\n",
    "    'lambda': ContinuousParameter(1e-5, 10),\n",
    "}\n",
    "\n",
    "estimator.set_hyperparameters(\n",
    "    eval_metric='auc,logloss,error',\n",
    "    num_round=1000,  # Set a large number for num_round and rely on early stopping\n",
    "    objective='binary:logistic',\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "# Specify the objective metric\n",
    "objective_metric_name = 'validation:auc'\n",
    "\n",
    "# Create and launch a hyperparameter tuning job\n",
    "tuner = HyperparameterTuner(estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=30,\n",
    "                            max_parallel_jobs=10)\n",
    "\n",
    "# Specify the training and validation data locations and content type\n",
    "train_input = TrainingInput('s3://diabeterstraindata/sagemaker-data/kc/train.csv', content_type='text/csv')\n",
    "validation_input = TrainingInput('s3://diabeterstraindata/sagemaker-data/kc/val.csv', content_type='text/csv')\n",
    "\n",
    "# Launch a hyperparameter tuning job\n",
    "tuner.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4302dc8b",
   "metadata": {},
   "source": [
    "### Retrieving metrics of model tuning\n",
    "\n",
    "- We initialize a SageMaker session, which provides a managed environment within Amazon SageMaker for managing training jobs, endpoints, and other SageMaker resources.\n",
    "- We retrieve the name of the best training job resulting from a hyperparameter tuning process using the best_training_job() method of the tuner object. This job is identified as the one that achieved the best performance on the specified objective metric during the tuning process.\n",
    "- We then obtain detailed information about this best training job by invoking the describe_training_job() method of the SageMaker session, passing in the name of the best training job. - - This method returns a dictionary containing comprehensive details about the training job, including its hyperparameters, input data configuration, and metrics.\n",
    "- The hyperparameters used by the best training job are accessed from the 'HyperParameters' key of the dictionary returned by describe_training_job(). We print these hyperparameters to understand which values led to the best model performance.\n",
    "- We also retrieve and print the metrics associated with the best training job, specifically looking for metrics related to model validation such as 'validation:auc', 'validation:logloss', and 'validation:error'. These metrics help us evaluate how well the model performed on the validation dataset, giving insights into its generalization ability and predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "50cd08e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters are: \n",
      " {'_tuning_objective_metric': 'validation:auc', 'alpha': '0.8414728374508231', 'colsample_bytree': '0.5322761211279915', 'early_stopping_rounds': '10', 'eta': '0.17491253458439093', 'eval_metric': 'auc,logloss,error', 'gamma': '4.562856759911896', 'lambda': '0.0002383571851544438', 'max_depth': '5', 'min_child_weight': '7.7352942236405875', 'num_round': '1000', 'objective': 'binary:logistic', 'subsample': '0.9211582162396529'}\n",
      "\n",
      "The following metrics are from the best model:\n",
      "validation:logloss: 0.08240800350904465\n",
      "validation:auc: 0.9786859750747681\n",
      "validation:error: 0.02889999933540821\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get the name of the best training job from hyperparameter tuning\n",
    "best_training_job_name = tuner.best_training_job()\n",
    "\n",
    "# Now get the details of the best training job\n",
    "best_training_job_info = sagemaker_session.describe_training_job(best_training_job_name)\n",
    "\n",
    "# The hyperparameters of the best training job are in the 'HyperParameters' key\n",
    "best_hyperparameters = best_training_job_info['HyperParameters']\n",
    "\n",
    "print(f\"The best hyperparameters are: \\n {best_hyperparameters}\")\n",
    "\n",
    "# Describe the best training job to get the metrics\n",
    "metrics = best_training_job_info['FinalMetricDataList']\n",
    "\n",
    "print(\"\\nThe following metrics are from the best model:\")\n",
    "# Print out the metrics for the best training job\n",
    "for metric in metrics:\n",
    "    if metric['MetricName'] in ['validation:auc', 'validation:logloss', 'validation:error']:\n",
    "        print(f\"{metric['MetricName']}: {metric['Value']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a666be7-dcac-44e5-8b72-d8df97b333d1",
   "metadata": {},
   "source": [
    "### Interpretation of metrics\n",
    "\n",
    "- The **hyperparameter tuning process has optimized the model** to achieve a validation AUC of 0.978689948425293, indicating our model has excellent capability in distinguishing between the classes for the problem at hand. The tuning results show that our model has a strong performance on the validation set, with low log loss (0.08252300322055817) and error (0.02920000688433467), suggesting high accuracy and good calibration of predicted probabilities. This model is expected to generalize well to new data, based on the validation metrics provided.\n",
    "- The selected hyperparameters include a **significant alpha (L1 regularization term) of 0.32256479401649434 and a substantial lambda (L2 regularization term) of 0.7578470196080217**, suggesting a model that is **robust against overfitting**, by applying both L1 and L2 regularization effectively.\n",
    "- A **gamma value of 3.5499706277804544** specifies the minimum loss reduction required to make a further partition on a leaf node of the tree. This relatively high value means the model is more conservative and less prone to overfitting.\n",
    "- The **learning rate (eta) is 0.11035511928866**, which is a moderate value, balancing the speed of learning and the risk of overfitting by controlling the contribution of each tree to the final model.\n",
    "- The **maximum depth of the trees, 'max_depth': '8', is relatively deep,** allowing the model to capture complex patterns in the data, which could be beneficial given the complex nature of healthcare data.\n",
    "- The **'min_child_weight' of 9.837971841334964** is **relatively high**, which can help prevent overfitting by making the algorithm more conservative and requiring a significant number of instances to make a child node.\n",
    "- The **'subsample' rate is 0.9180113714220905**, suggesting that each tree is built using approximately 91.8% of the data, which **helps in preventing overfitting** by adding more randomness into the model.\n",
    "- The **'colsample_bytree' parameter is set to 0.55858184126595706**, indicating that each tree uses around 55.9% of features, allowing the model to perform feature sampling, providing a diversity of trees and further guarding against overfitting.\n",
    "- 'num_round' (the number of boosting rounds) is 1000, indicating a **substantial model complexity and potential for learning intricate patterns in the data.**\n",
    "- The early **stopping parameter 'early_stopping_rounds': '10'** means that the **model training will stop if the validation metric does not improve for 10 consecutive rounds,** helping to prevent overfitting and unnecessary computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd7a72d",
   "metadata": {},
   "source": [
    "## Deploying the model\n",
    "\n",
    "Now that the model is ready, we can \"deploy it\". This will create an instance that \"serves\" the model continuosuly. This server will accept queries with input values in real time and will return the model prediction. \n",
    "\n",
    "- We create a new estimator object by attaching it to the best training job using the Estimator.attach() method from the SageMaker SDK. This method allows us to rehydrate an estimator object from a completed training job, in this case, the one identified as the best from the hyperparameter tuning process.\n",
    "- We deploy the best model (from the best training job) to a SageMaker endpoint. To do this, we call the deploy() method on the best_estimator object, specifying the initial number of instances (initial_instance_count=1), the type of instance (instance_type='ml.m4.xlarge'), and the serializer (serializer=sagemaker.serializers.CSVSerializer()). The serializer is configured to format the incoming data as CSV before making predictions.\n",
    "- This process results in the creation of a SageMaker endpoint based on the model from the best training job. This endpoint can then be used to make real-time predictions by sending it data in the CSV format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fccc37-e468-48fe-b3c7-db98c45579a7",
   "metadata": {},
   "source": [
    "### SageMaker Endpoint Limitation and Resolution\n",
    "\n",
    "In our project, we are aware of AWS SageMaker's service limits on the number of model endpoints and instance types we can deploy. These limits are designed to help manage AWS resource allocation and control costs. However, encountering these limits can halt project progress by preventing the creation of new model endpoints.\n",
    "\n",
    "#### How to Solve the Issue\n",
    "There are three primary methods to address a service limit issue:\n",
    "\n",
    "1. **Terminate Unused Endpoints**\n",
    "2. **Request a Service Limit Increase**\n",
    "3. **Use an Alternative Instance Type**\n",
    "\n",
    "For the purposes of this project, we have chosen to adopt the first method: terminating unused endpoints. This approach is often the quickest way to free up resources and continue working without waiting for limit increases or changing instance types.\n",
    "\n",
    "If you're using the project and encounter a `ResourceLimitExceeded` error, indicating you've hit the limit for the number of deployable endpoints, follow these steps to resolve the issue:\n",
    "\n",
    "1. **Terminate Unused Endpoints**:\n",
    "   - Open the [SageMaker console](https://console.aws.amazon.com/sagemaker/).\n",
    "   - In the navigation pane, select **Endpoints**.\n",
    "   - Review the list of endpoints to identify any that are no longer in use.\n",
    "   - Select the endpoint(s) you wish to terminate.\n",
    "   - Click on **Actions**, then choose **Delete** to remove the endpoint and free up resources.\n",
    "\n",
    "By regularly reviewing and managing your SageMaker endpoints, you can ensure that you stay within the service limits and avoid potential disruptions to your workflow. Should you require more resources for your project in the future, consider requesting a service limit increase or using alternative instance types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "870f45c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2024-04-02 22:11:16 Starting - Found matching resource for reuse\n",
      "2024-04-02 22:11:16 Downloading - Downloading the training image\n",
      "2024-04-02 22:11:16 Training - Training image download completed. Training in progress.\n",
      "2024-04-02 22:11:16 Uploading - Uploading generated training model\n",
      "2024-04-02 22:11:16 Completed - Resource retained for reuse"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2024-04-02-22-15-14-490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name sagemaker-xgboost-2024-04-02-22-15-14-490\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-xgboost-2024-04-02-22-15-14-490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "# If we had not done any hyperparameter optimization, we woudl be using the following command:\n",
    "# predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge', serializer=sagemaker.serializers.CSVSerializer())\n",
    "\n",
    "# Create a new estimator object from the best training job\n",
    "best_estimator = sagemaker.estimator.Estimator.attach(best_training_job_name)\n",
    "\n",
    "# Now, deploy the best model\n",
    "predictor = best_estimator.deploy(initial_instance_count=1,\n",
    "                                  instance_type='ml.m5.xlarge',\n",
    "                                  serializer=sagemaker.serializers.CSVSerializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9012956",
   "metadata": {},
   "source": [
    "### Predicting a patient\n",
    "\n",
    "#### One patient\n",
    "To predict a new patient, we can input normalizued feature data and hit predict, with the outcome showing the probability of the patient having diabetes (closer to 1 means a high probability).\n",
    "\n",
    "The output we are seeing, such as b'0.0003831279755104333', represents the model's predicted probability that the given observation belongs to class 0 (whichaccording to our coding of the classes, refers to \"no diabetes\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ae877f4-a134-4df1-8b4e-b4052399a52b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'0.00035908748395740986'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(\"-0.081830,-0.284208,-0.202524,-0.001838,-0.492907,-0.295076,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23ae7c5-0e63-4302-9673-24e9133dbc5c",
   "metadata": {},
   "source": [
    "#### Multiple patients at the same time (taken from the test set)\n",
    "\n",
    "In order to predict more than one patient, we first need to preprocess the input data:\n",
    "\n",
    "- We create a subset of the test dataset (X_test) by selecting the first 5 rows. This subset is intended for making predictions and demonstrating the prediction process on a smaller scale.\n",
    "- We print the subset of X_test to the console, displaying the selected rows along with their column names. This step is useful for understanding the data before it undergoes preprocessing.\n",
    "- The subset is then preprocessed using the previously defined preprocessor pipeline. This preprocessing includes any transformations specified earlier, such as imputation, scaling, and encoding, to prepare the data for the model.\n",
    "- Since the preprocessing steps can result in a sparse matrix (especially after encoding categorical variables), we convert this sparse matrix to a dense format using the toarray() method. This conversion is necessary because some models or subsequent steps may require a dense format.\n",
    "- We convert the dense array of preprocessed data into a pandas DataFrame. This step is useful for handling the data in a structured format and potentially applying further transformations or analysis.\n",
    "- We prepare the preprocessed DataFrame for prediction by converting it to CSV format. This involves writing the DataFrame to an in-memory text stream (io.StringIO()) without headers and index columns, which is a common requirement for model prediction inputs. The resulting CSV format data is stored in a variable for easy access or transmission, for instance, when sending the data to a machine learning model endpoint for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91ded757-ec63-47b1-8758-9c382a7eb682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset of X_test before preprocessing:\n",
      "       gender   age  hypertension  heart_disease smoking_history    bmi  \\\n",
      "65074  Female  50.0             0              0         No Info  33.06   \n",
      "46427  Female  46.0             0              0         No Info  27.32   \n",
      "31779  Female   4.0             0              0         No Info  27.32   \n",
      "33827    Male   7.0             0              0     not current  19.47   \n",
      "21868  Female  43.0             0              0     not current  30.51   \n",
      "\n",
      "       HbA1c_level  blood_glucose_level  \n",
      "65074          6.0                  145  \n",
      "46427          3.5                  126  \n",
      "31779          6.1                  158  \n",
      "33827          3.5                  130  \n",
      "21868          6.1                  158  \n"
     ]
    }
   ],
   "source": [
    "# Create subset of X_test to predict\n",
    "subset_X_test = X_test.iloc[:5]\n",
    "\n",
    "# Display the subset of X_test with column names\n",
    "print(\"Subset of X_test before preprocessing:\")\n",
    "print(subset_X_test)\n",
    "\n",
    "# Preprocess the subset\n",
    "subset_X_test_preprocessed = preprocessor.transform(subset_X_test)\n",
    "\n",
    "# Create a DataFrame for the preprocessed data\n",
    "subset_X_test_preprocessed_df = pd.DataFrame(subset_X_test_preprocessed)\n",
    "\n",
    "# Convert the preprocessed DataFrame to CSV format for prediction\n",
    "placeholder = io.StringIO()\n",
    "subset_X_test_preprocessed_df.to_csv(placeholder, header=False, index=False)\n",
    "csv_data = placeholder.getvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab5220f-65fe-4df7-9195-d51574446e33",
   "metadata": {},
   "source": [
    "In a next step, we then take the preprocessed data and run the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cccc29ed-3ca3-498f-9c3f-7429501ef799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'0.06106920167803764,0.0004101478843949735,0.0009868874913081527,0.00017709078383632004,0.004902362357825041'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict data with deployed model endpoint\n",
    "predictions = predictor.predict(csv_data)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c8c7f3-c75a-4fc1-b249-992de1f917dd",
   "metadata": {},
   "source": [
    "Lastly, we generate IDs for the inidividual patients we are predicting and create a datafrmae with their IDS and respective predictions. We then convert it into a csv file and export it to out predictions S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "723d94d4-fa2f-42d8-903e-dad99ebf0deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities = np.fromstring(predictions, sep=',')\n",
    "predicted_labels = (predicted_probabilities >= 0.5).astype(int)\n",
    "\n",
    "# Generate sequential patient IDs\n",
    "PolicyHolder = range(1, len(predicted_labels) + 1)\n",
    "\n",
    "# Create a DataFrame with generated patient IDs and their corresponding predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    \"PolicyHolderID\": PolicyHolder,\n",
    "    \"Prediction\": predicted_labels\n",
    "})\n",
    "\n",
    "# Define the path for saving the CSV\n",
    "predictions_csv = \"predictions.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "predictions_df.to_csv(predictions_csv, index=False)\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "bucket_name = 'diabetesstorage'\n",
    "\n",
    "# Upload predictions CSV\n",
    "predictions_s3_path = 'predictions/predictions.csv'\n",
    "s3_client.upload_file(predictions_csv, bucket_name, predictions_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5799e71f-01fd-40c4-b38f-c1b2962bf94d",
   "metadata": {},
   "source": [
    "## Model generalization performance evaluation\n",
    "\n",
    "To assess the model's **practical impact and usefulness**, we will **assess its performacne against the test set**. The generated confusion matrix will help us claculate the financial impact of the model.\n",
    "- We import various performance metrics from the sklearn.metrics module, including accuracy, precision, recall, F1 score, ROC AUC score, and the confusion matrix. These metrics are crucial for evaluating the performance of a classification model.\n",
    "- A pandas DataFrame is created for the preprocessed test data, which we had transformed earlier using a preprocessing pipeline to fit the model's requirements (see above).\n",
    "- We convert this DataFrame to CSV format (without headers or indexes) and use it to make predictions with the predictor object. The predictor refers to a deployed model endpoint that can accept input data and return predictions. The predictions are decoded from UTF-8 format to a string.\n",
    "- The prediction results, in the form of probabilities, are converted from a string to a NumPy array. Based on these probabilities, we determine the predicted labels by applying a threshold: values equal to or above 0.5 are classified as 1 (having diabetes), and below 0.5 are classified as 0 (no diabetes).\n",
    "- We calculate the confusion matrix using the true labels (y_test) and the predicted labels. The confusion matrix provides a detailed breakdown of the model's predictions, including true positives, true negatives, false positives, and false negatives.\n",
    "- Various performance metrics are computed to evaluate the model, including accuracy, precision, recall, F1 score, and ROC AUC score. The ROC AUC score is calculated using the predicted probabilities rather than the binary labels to assess the model's ability to distinguish between classes.\n",
    "- Finally, we print the calculated metrics and the confusion matrix to summarize the model's performance on the test dataset. These metrics provide a comprehensive overview of the model's predictive accuracy and its strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d78dcfaa-4108-434a-bc49-86f83e15bfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9711\n",
      "Precision: 0.9815\n",
      "Recall: 0.6775\n",
      "F1 Score: 0.8016\n",
      "ROC AUC Score: 0.9785\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative                9127                  11\n",
      "Actual Positive                 278                 584\n",
      "\n",
      "Total number of predictions made: 10000\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame for the preprocessed test data\n",
    "X_test_preprocessed_df = pd.DataFrame(X_test_preprocessed)\n",
    "\n",
    "# Convert to CSV format and predict\n",
    "predictions = predictor.predict(X_test_preprocessed_df.to_csv(header=False, index=False)).decode('utf-8')\n",
    "\n",
    "predicted_probabilities = np.fromstring(predictions, sep=',')\n",
    "predicted_labels = (predicted_probabilities >= 0.5).astype(int)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, predicted_labels)\n",
    "\n",
    "# Label the confusion matrix\n",
    "cm_df = pd.DataFrame(cm, \n",
    "                     index=['Actual Negative', 'Actual Positive'], \n",
    "                     columns=['Predicted Negative', 'Predicted Positive'])\n",
    "\n",
    "# Compute various performance metrics\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "precision = precision_score(y_test, predicted_labels)\n",
    "recall = recall_score(y_test, predicted_labels)\n",
    "f1 = f1_score(y_test, predicted_labels)\n",
    "roc_auc = roc_auc_score(y_test, predicted_probabilities)  # Use predicted probabilities for ROC AUC\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Print the labeled confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm_df)\n",
    "\n",
    "# Output the total number of predictions made\n",
    "total_predictions = len(predicted_labels)  # or use len(y_test)\n",
    "print(f\"\\nTotal number of predictions made: {total_predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05893c0c-010d-4e51-b07a-5fea4d00ca41",
   "metadata": {},
   "source": [
    "You will find the interpretation of these results in our presentation. We lastly convert and **export our retrieved model performance metrics to out repective S3 bucket.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "891c43ea-0602-4c42-9668-813410bf0818",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC AUC Score\"],\n",
    "    \"Value\": [accuracy, precision, recall, f1, roc_auc]\n",
    "})\n",
    "\n",
    "metrics_csv = \"metrics.csv\"\n",
    "metrics_df.to_csv(metrics_csv, index=False)\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "bucket_name = 'diabetesstorage'\n",
    "\n",
    "# Upload metrics CSV\n",
    "metrics_s3_path = 'metrics/metrics.csv'\n",
    "s3_client.upload_file(metrics_csv, bucket_name, metrics_s3_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
